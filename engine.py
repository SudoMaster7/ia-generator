import torch
from diffusers import StableDiffusionPipeline, AutoencoderKL
from PIL import Image
import io
import base64
import gc
import time
import os
import re
import requests # Necess√°rio para API Premium
from datetime import datetime

class ImageGenerator:
    def __init__(self):
        self.current_progress = 0
        self.current_device = "cuda" if torch.cuda.is_available() else "cpu"
        self.pipe = None
        self.stop_signal = False
        self.load_model(self.current_device)

    def load_model(self, target_device):
        print(f"--- Carregando modelo no dispositivo: {target_device} ---")
        
        # Limpa mem√≥ria antiga
        if self.pipe is not None:
            del self.pipe
            self.pipe = None
            gc.collect()
            torch.cuda.empty_cache()

        # Se for API Premium, n√£o carrega modelo local
        if target_device == "nano-banana":
            self.current_device = target_device
            print("--- Modo API Premium (Nano Banana) Ativado ---")
            return

        model_id = "runwayml/stable-diffusion-v1-5"
        
        try:
            if target_device == "cuda":
                from diffusers import DPMSolverMultistepScheduler

                # 1. Carrega em Float32 (Elimina Tela Preta e Erros de Tipo)
                self.pipe = StableDiffusionPipeline.from_pretrained(
                    model_id, 
                    torch_dtype=torch.float32, # Precis√£o total para estabilidade
                    use_safetensors=True,
                    safety_checker=None, 
                    requires_safety_checker=False
                )

                # 2. Scheduler DPM++ (Recupera a velocidade)
                # Permite usar steps=20 com alta qualidade
                self.pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                    self.pipe.scheduler.config, 
                    use_karras_sigmas=True
                )

                # 3. Otimiza√ß√£o de Mem√≥ria R√°pida
                # 'model_cpu_offload' √© muito melhor que 'sequential'.
                # Ele mant√©m o modelo pronto na RAM e joga para a VRAM instantaneamente.
                self.pipe.enable_model_cpu_offload()
                
                # Otimiza√ß√µes extras
                self.pipe.enable_vae_tiling() 
                self.pipe.enable_attention_slicing()
                
            else:
                # CPU Fallback
                self.pipe = StableDiffusionPipeline.from_pretrained(
                    model_id, 
                    use_safetensors=True,
                    safety_checker=None,
                    requires_safety_checker=False
                )
                self.pipe.to("cpu")
            
            self.current_device = target_device
            print("--- Modelo carregado (Modo Compatibilidade Total) ---")
            
        except Exception as e:
            print(f"Erro ao carregar modelo: {e}")

    def switch_device(self, device_name):
        if device_name != self.current_device:
            self.current_device = device_name
            self.load_model(device_name)
        return self.current_device

    def _save_to_disk(self, image, prompt):
        folder_name = "imagens_geradas"
        if not os.path.exists(folder_name):
            os.makedirs(folder_name)

        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        # Limita o nome do arquivo para evitar erros, mas salva o prompt inteiro no txt
        safe_prompt = re.sub(r'[\\/*?:"<>|]', "", prompt)[:30].replace(" ", "_")
        filename = f"{timestamp}_{safe_prompt}.png"
        
        path = os.path.join(folder_name, filename)
        image.save(path)
        
        # --- NOVO: Salva o prompt completo em um arquivo de texto ---
        txt_filename = f"{timestamp}_{safe_prompt}.txt"
        txt_path = os.path.join(folder_name, txt_filename)
        try:
            with open(txt_path, "w", encoding="utf-8") as f:
                f.write(prompt)
        except Exception as e:
            print(f"Erro ao salvar prompt txt: {e}")
            
        print(f"Imagem salva em: {path}")


    def _generate_premium(self, prompt):
        """Gera imagem usando a API Premium (Nano Banana / Pollinations)"""
        print(f"üçå Gerando via Nano Banana Premium (Modelo FLUX): '{prompt}'...")
        start_time = time.time()
        
        try:
            # Usando Pollinations.ai com o modelo FLUX (Melhor qualidade)
            seed = int(time.time())
            # Modelo Flux, resolu√ß√£o HD (1024x1024), enhance=true para melhorar prompt
            url = f"https://image.pollinations.ai/prompt/{prompt}?model=flux&width=1024&height=1024&seed={seed}&nologo=true&enhance=true"
            
            # Aumentado timeout para 60s para evitar erros de leitura
            response = requests.get(url, timeout=60)
            response.raise_for_status()
            
            image = Image.open(io.BytesIO(response.content))
            
            # Salva localmente para hist√≥rico
            self._save_to_disk(image, prompt)
            
            end_time = time.time()
            duration = round(end_time - start_time, 2)
            
            buffered = io.BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
            
            return {
                "image": img_str,
                "duration": duration,
                "device": "NANO BANANA (CLOUD)",
                "steps": "N/A"
            }
            
        except Exception as e:
            print(f"Erro na API Nano Banana: {e}")
            return None

    def cancel(self):
        """Ativa o sinal de parada"""
        print("!!! SINAL DE CANCELAMENTO RECEBIDO !!!")
        self.stop_signal = True

    def generate(self, prompt, steps=20): # Reduzi steps padr√£o para 20 para testar mais r√°pido
        if self.current_device == "nano-banana":
            return self._generate_premium(prompt)

        try:
            self.current_progress = 0
            start_time = time.time()
            
            def callback_fn(step, timestep, latents):
                self.current_progress = int((step / steps) * 100)
            
            print(f"Gerando '{prompt}'...")
            
            # Limpa cache for√ßado
            gc.collect()
            torch.cuda.empty_cache()

            image = self.pipe(
                prompt, 
                num_inference_steps=steps, 
                callback=callback_fn, 
                callback_steps=1
            ).images[0]
            
            self._save_to_disk(image, prompt)

            end_time = time.time()
            duration = round(end_time - start_time, 2)
            self.current_progress = 100
            
            buffered = io.BytesIO()
            image.save(buffered, format="PNG")
            img_str = base64.b64encode(buffered.getvalue()).decode("utf-8")
            
            return {
                "image": img_str,
                "duration": duration,
                "device": self.current_device.upper(),
                "steps": steps
            }
            
        except RuntimeError as e:
            if "CANCELLED_BY_USER" in str(e):
                print("--- Gera√ß√£o abortada com sucesso ---")
                return {"status": "cancelled"}
            print(f"Erro de Runtime: {e}")
            return {"status": "error", "message": str(e)}
            
        except Exception as e:
            print(f"Erro fatal na gera√ß√£o: {e}")
            return {"status": "error", "message": str(e)}